\chapter{Extracting and Analyzing Results}

\glc\ stores its output in an \href{http://www.hdfgroup.org/HDF5/}{HDF5} file. The contents of this file can be viewed and manipulated using a variety of ways including:
\begin{description}
 \item[\href{http://www.hdfgroup.org/hdf-java-html/hdfview/}{{\sc HDFView}}] This is a graphical viewer for exploring the contents of HDF5 files;
 \item[\href{http://www.hdfgroup.org/products/hdf5_tools/index.html\#h5dist}{HDF5 Command Line Tools}] A set of tools which can be used to extract data from HDF5 files (\href{http://www.hdfgroup.org/HDF5/doc/RM/Tools.html#Tools-Dump}{{\tt h5dump}} and \href{http://www.hdfgroup.org/HDF5/doc/RM/Tools.html#Tools-Ls}{{\tt h5ls}} are particularly useful);
 \item[\href{http://www.hdfgroup.org/HDF5/doc/RM/RM_H5Front.html\#F90andCPPlus}{C++ and Fortran 90 APIs}] Allow access to and manipulation of data in HDF5 files;
 \item[\href{http://code.google.com/p/h5py/}{{\sc h5py}}] A Python interface to HDF5 files.
\end{description}

In the remainder of this section the structure of \glc\ HDF5 files is described and a general-purpose Perl module which we use to extract data in a convenient manner is outlined.

\section{General Structure of Output File}

Figure~\ref{fig:glcOutputFileStructure} shows the structure of a typical \glc\ output file. The various groups and subgroups are described below.

\begin{figure}
\begin{center}
\begin{verbatim}
outputFile.hdf5
 |
 +-> Outputs                                  Group
 |    |
 |    +-> Output1                             Group
 |    |    |
 |    |    +-> mergerTree1                    Group
 |    |    |     |
 |    |    |     +-> nodeProperty1            Dataset {1002/Inf}
 |    |    |     +-> ...                      Dataset {1002/Inf}
 |    |    |     +-> ...                      Dataset {1002/Inf}
 |    |    |     +-> ...                      Dataset {1002/Inf}
 |    |    |     +-> nodePropertyN            Dataset {1002/Inf}
 |    |    |
 |    |    x-> ...                            Group
 |    |    x-> ...                            Group
 |    |    x-> ...                            Group
 |    |    x-> mergerTreeN                    Group
 |    |    |
 |    |    +-> outputExpansionFactor          Dataset {1}
 |    |    +-> outputTime                     Dataset {1}
 |    |
 |    x-> Output2                             Group
 |
 +-> Parameters                               Group
 |    |
 |    +-> inputParameter1                     Dataset {1}
 |    +-> ...                                 Dataset {1}
 |    +-> ...                                 Dataset {1}
 |    +-> ...                                 Dataset {1}
 |    +-> inputParameterN                     Dataset {1}
 |
 +-> Version                                  Group
 |    |
 |    +-> runTime                             Dataset {1}
 |    +-> versionMajor                        Dataset {1}
 |    +-> versionMinor                        Dataset {1}
 |    +-> versionRevision                     Dataset {1}
 |    +-> bazaarRevision                      Dataset {1}
 |    +-> runByName                           Dataset {1}
 |    +-> runByEmail                          Dataset {1}
 |
 +-> globalHistory                            Group
      |
      +-> historyExpansion                    Dataset {30}
      +-> historyStarFormationRate            Dataset {30}
      +-> historyTime                         Dataset {30}
\end{verbatim}
\end{center}
\caption{Structure of a \glc\ HDF5 output file.}
\label{fig:glcOutputFileStructure}
\end{figure}

\subsection{Parameters}

The {\tt Parameters} group contains a record of all parameter values (either input or default) that were used for this \glc\ run. The group contains a long list of datasets, each dataset named for the corresponding parameter and with a single entry giving the value of that parameter. The {\tt scripts/aux/Extract\_Parameter\_File.pl} script can be used to extract these parameter values to an XML file suitable for re-input into \glc.

\subsection{Version}

The {\tt Version} group contains a record of the \glc\ version used for this model, storing the major and minor version numbers, the revision number and the {\sc Bazaar} revision (if the code is being maintained using {\sc Bazaar}, otherwise a value of $-1$ is entered). Additionally, the time at which the model was run is stored and, if the {\tt galacticusConfig.xml} file (see \S\ref{sec:ConfigFile}) is present and contains contact details, the name and e-mail address of the person who ran the model.

\subsection{globalHistory}

The {\tt globalHistory} group stores volume averaged properties of the model universe as a function of time. Currently, the properties stored are:
\begin{description}
 \item[{\tt historyTime}] Cosmic time (in Gyr);
 \item[{\tt historyExpansion}] Expansion factor;
 \item[{\tt historyStarFormationRate}] Volume averaged star formation rate (in $M_\odot/$Gyr/Mpc$^3$).
 \item[{\tt historyStellarDensity}] Volume averaged stellar mass density (in $M_\odot/$Mpc$^3$).
 \item[{\tt historyGasDensity}] Volume averaged cooled gas density (in $M_\odot/$Mpc$^3$).
 \item[{\tt historyNodeDensity}] Volume averaged resolved node density (in $M_\odot/$Mpc$^3$).
\end{description}

\subsection{Outputs}

The {\tt Outputs} group contains one or more sub-groups corresponding to the output times requested from \glc. Each sub-group contains the following information:
\begin{description}
 \item[{\tt outputTime}] The cosmic time (in Gyr) at this output;
 \item[{\tt outputExpansionFactor}] The expansion factor at this output;
 \item[{\tt mergerTree} subgroups] A set of {\tt mergerTree} groups as described below.
\end{description}

\subsubsection{mergerTree subgroups}

Each {\tt mergerTree} subgroup contains all data on a single merger tree. The group consists of a collection of datasets each of which lists a property of all nodes in the tree which exist at the output time. Additionally, the {\tt volumeWeight} dataset gives the weight (in Mpc$^{-3}$) which should be assigned to this tree (and all nodes in it) to create a volume-averaged sample.

\subsection{Optional Outputs}

Numerous other quantities can be optionally output. These are documented below:

\subsubsection{Mass Accretion Histories}

A mass accretion history (i.e. mass as a function of time) for the main branch in each merger tree can be output by setting {\tt massAccretionHistoryOutput}$=${\tt true}. If requested, a new group {\tt massAccretionHistories} will be made in the \glc\ output file. It will contain groups called {\tt mergerTreeN} where {\tt N} is the merger tree index. Each such group will contain the following three datasets, defined for the main branch of the tree\footnote{``Main branch'' is defined by starting from the root node of a tree and repeatedly stepping back to the most massive progenitor of the branch. This does not necessarily pick out the most massive progenitor at a given time.}:
\begin{description}
 \item [{\tt nodeIndex}] The index of the node in the tree;
 \item [{\tt nodeTime}] The time at this point in the tree (in Gyr);
 \item [{\tt nodeMass}] The mass of the node at this point in the tree (in $M_\odot$).
\end{description}

\subsubsection{Pre-Evolution Merger Trees}

\glc\ can output the full structure of merger trees prior to any evolution. Merger tree structure can be requested by setting {\tt mergerTreeStructureOutput}$=${\tt true}. Structures are written to a new group, {\tt mergerTreeStructures}, in the \glc\ output file. This group will contain groups called {\tt mergerTreeN} where {\tt N} is the merger tree index. Each such group will contain the following datasets:
\begin{description}
 \item [{\tt nodeIndex}] The index of the node in the tree;
 \item [{\tt childNodeIndex}] The index of this node's first child node;
 \item [{\tt parentNodeIndex}] The index of this node's parent node;
 \item [{\tt siblingNodeIndex}] The index of this node's sibling node;
 \item [{\tt nodeTime}] The time at this point in the tree (in Gyr);
 \item [{\tt nodeMass}] The mass of the node at this point in the tree (in $M_\odot$).
\end{description}
Additional, optional, datasets can be added by setting appropriate input parameters. Currently these include:
\begin{itemize}
 \item [Virial quantities] If {\tt mergerTreeStructureOutputVirialQuantities}$=${\tt true} then two additional datasets are included:
 \begin{description}
  \item [{\tt nodeVirialRadius}] The virial radius of the node (in Mpc);
  \item [{\tt nodeVirialVelocity}] The virial velocity of the node (in km/s);
 \end{description}
 \item [Dark matter scale radii] If {\tt mergerTreeStructureOutputDarkMatterScaleRadius}$=${\tt true} then an additional dataset is included:
 \begin{description}
  \item [{\tt darkMatterScaleRadius}] The scale radius of this node's dark matter halo profile (in Mpc);
 \end{description}
\end{itemize}

\section{Perl Module for Data Extraction}

A Perl module is provided that allows for easy extraction of datasets from the \glc\ output file together with a straightforward way to implement derived properties. To use this Perl module, add
\begin{verbatim}
 use lib "./perl";
 use PDL;
 use Galacticus::HDF5;
\end{verbatim}
at the start of your Perl script. The {\tt Galacticus::HDF5} module will import data from a \glc\ HDF5 file into PDL variables. All data are stored in a single structure, which also specifies the file, output and range of trees to read. An example of reading a dataset from a file is:
\begin{verbatim}
 $dataSet{'file'} = "galacticus.hdf5";
 $dataSet{'output'} = 1;
 $dataSet{'tree'} = "all";
 $dataSet{'dataRange'} = [1,2];
 $dataSet{'store'} = 0;
 &HDF5::Get_Dataset(\%dataSet,['nodeMass']);
 $dataSets = \%{$dataSet{'dataSets'}};
 print ${$dataSets->{'nodeMass'}}."\n";
\end{verbatim}
The {\tt \%dataSet} hash is initialized with information to specify which file, output and trees should be used. Its settable components are:
\begin{description}
 \item [{\tt file}] The name of the \glc\ output file to be read.
 \item [{\tt output}] Specify the output number in the file which should be read.
 \item [{\tt tree}] Specify the tree which should be read, or use ``all'' to specify that all trees be read.
 \item [{\tt dataRange}] Gives the first and last entry in the dataset to read---this facilitates reading of partial datasets (and therefore reading datasets in a piecemeal fashion). If this component is missing, the entire dataset is read.
 \item[{\tt store}] If set to 1, any derived properties will be stored back in the \glc\ output file for later retrieval. If set to 0 (or if this option is not present), derived properties will not be stored. Currently, storing of derived properties in the \glc\ file is only possible if the {\tt tree} option is set to ``all'' and no {\tt dataRange} is specified.
\end{description}
The {\tt \&HDF5::Get\_Dataset($\backslash$\%dataSet,['nodeMass']);} call requests that the {\tt nodeMass} dataset be read. It is return as a PDL variable in the {\tt nodeMass} element of the {\tt dataSets} hash which is itself a member of {\tt \$dataSet}. The final lines in the example simply write out the resulting array of {\tt nodeMass} values.

\subsection{Derived Properties}

Derived properties can be created by giving defining functions along with a regular expression string that allows them to be matched. For example, the {\tt Galacticus::Baryons} module implements a hot gas fraction property called {\tt hotHaloFraction} or {\tt hotHaloFrac}. It has the following form:
\begin{verbatim}
package Baryons;
use PDL;
use Galacticus::HDF5;
use Data::Dumper;

%HDF5::galacticusFunctions = ( %HDF5::galacticusFunctions,
    "hotHalo(Fraction|Frac)" => "Baryons::Get_hotHaloFraction"
    );

my $status = 1;
$status;

sub Get_hotHaloFraction {
    $dataSet = shift;
    $dataSetName = $_[0];
    &HDF5::Get_Dataset($dataSet,['hotHaloMass','nodeMass']);
    $dataSets = \%{${$dataSet}{'dataSets'}};
    ${$dataSets->{$dataSetName}} = ${$dataSets->{'hotHaloMass'}}/${$dataSets->{'nodeMass'}};
}

\end{verbatim}
The module begins by adding an entry to the {\tt \%HDF5::galacticusFunctions} hash. The key gives a regular expression which matches to the name of the property to be defined. The value of the key gives the name of a subroutine to be called to evaluate this expression. The subroutine is defined below. When called, it receives the {\tt \$dataSet} structure along with the name of the requested property. The subroutine should then simply evaluate the requested property and store it in the appropriate location within {\tt \$dataSet}. Note that the subroutine can request additional datasets be loaded (as happens above where {\tt hotHaloMass} and {\tt nodeMass} are requested) if they are needed for its calculations.

\subsubsection{Available Derived Properties}

\begin{description}
 \item[{\tt mergerTreeIndex}] The index of the merger tree in which the galaxy is found. Provided by: {\tt Galacticus::HDF5}.
 \item[{\tt hotHalo(Fraction\textbar Frac)}] The fraction the node's mass in the hot gas halo. Provided by: {\tt Galacticus::Baryons}.
 \item[{\tt inclination}] A randomly selected inclination for the disk (in degrees). Provided by: {\tt Galacticus::Inclination}.
 \item[{\tt \textasciicircum(disk\textbar bulge)StellarLuminosity:.*:dustAtlas($\backslash$[faceOn$\backslash$])\$}] Dust-extingiushed luminosities for disk and bulge found by interpolating in the dust tables of \cite{ferrara_atlas_1999}. If the {\tt [faceOn]} qualifier is present, extinctions are computed assuming that the disk is observed face-on, otherwise a random inclination is used. Optionally, the dust atlas file to used can be specified via {\tt \$\{\$dataSet\}\{'dustAtlasFile'\}}. The available dust atlases span a limited range of spheroid sizes and central optical depths in their tabulations. Standard behavior is to extrapolate beyond the ends of these ranges. This can be controlled via {\tt \$\{\$dataSet\}\{'dustAtlasExtrapolateInSize'\}} and {\tt \$\{\$dataSet\}\{'dustAtlasExtrapolateInTau'\}} respectively, which can be set to {\tt yes}/{\tt no} (or, equivalently, 1/0). Provided by: {\tt Galacticus::DustAttenuation}.
 \item[{\tt \textasciicircum totalStellarLuminosity:.*:dustAtlas($\backslash$[faceOn$\backslash$])\$}] (Optionally dust-extingiushed) luminosities for disk plus bulge found by adding together the corresponding disk and bulge luminosities. Provided by: {\tt Galacticus::Luminosities}.
 \item[{\tt \textasciicircum bulgeToTotalLuminosity:.*:dustAtlas($\backslash$[faceOn$\backslash$])\$}] Ratio of bulge to total (optionally dust-extingiushed) luminosities. Provided by: {\tt Galacticus::Luminosities}.
 \item[{\tt \textasciicircum magnitude([\textasciicircum :]+):([\textasciicircum :]+):([\textasciicircum :]+):z([$\backslash$d$\backslash$.]+)(:dust[\textasciicircum :]+)?(:vega\textbar :AB)?}] Absolute magnitude corresponding to a stellar luminosity, in either Vega or AB systems. Provided by: {\tt Galacticus::Magnitudes}.
\end{description}

\section{Postprocessing Scripts}\label{sec:PostProcessingScripts}

\subsection{Model Rating}\label{sec:ModelRating}

While quantitative goodness-of-fit measures for a \glc\ model are useful it is often beneficial to be able to rate model quality ``by eye''. The {\tt scripts/aux/Rate\_Models.pl} script facilitates this. To rate models use:
\begin{verbatim}
 ./scripts/aux/Rate_Models.pl [option=....] [option=...] ...
\end{verbatim}
Allowed options are:
\begin{description}
 \item [{\tt modelDirectories}] [default={\tt models}] A comma-separated list of directories to be searched for models;
 \item [{\tt plotToRate}] [default={\tt Star\_Formation\_History.pdf}] The name of files containing a plot that you want to rate;
 \item [{\tt fitName}] [default={\tt Volume averaged star formation rate history}] A Perl regex name of the fit corresponding to these plots;
 \item [{\tt userName}] [default=<username>] The name of the person performing the rating.
\end{description}
The given model directories will be searched for files matching the name of the specified plot file. Each file found will be displayed on screen. The user can examine the plot and judge how good the fit is. Pressing {\tt q} will then close the image. The user may then enter a rating from 1--5 (higher values corresponding to better fits) by pressing the appropriate key. This rating (along with the name of the rater) will be stored in the corresponding fit data file. The net rating (an average of all ratings given to this model) is also stored.

\subsection{Model Sorting}\label{sec:ModelSorting}

When the {\tt Run\_Galacticus.pl} script (see \S\ref{sec:RunningGrids}) script is used to run grids of models and perform analysis on them it can be useful to be able to sort the resulting models on the basis of a goodness of fit measure. The {\tt scripts/aux/Sort\_Models.pl} provides this functionality. It will search one or more directories for \glc\ models and output an order list of models found, sorted by a goodness of fit measure. To run the script use:
\begin{verbatim}
 ./scripts/aux/Sort_Models.pl [option=....] [option=...] ...
\end{verbatim}
Allowed options are:
\begin{description}
 \item [{\tt modelDirectories}] [default={\tt models}] A comma-separated list of directories to be searched for models;
 \item [{\tt parametersToShow}] [default={\tt starFormationDiskEfficiency}] A comma-separated list of model parameter values to be included in the output;
 \item [{\tt sortOn}] [default=\verb@^net$@] A Perl regex to match the name of the fit-measure on which the sort should be performed;
 \item [{\tt useReducedChi2}] [default=1] Sort using $\chi^2$ if set to 0, or reduced $\chi^2$ if set to 1;
 \item [{\tt sortOnRating}] [default=1] Sort on the rating given to each model (see \S\ref{sec:ModelRating}) first, then on the $\chi^2$ value.
\end{description}

\subsection{Model Archiving}\label{sec:ModelArchiving}

It can be useful to store the results of \glc\ runs without storing the full output (e.g. when searching model parameter space it's useful to have a record of the goodness of fit at each point in space, but it's often useful to delete the full model output to save on disk space). The {\tt scripts/aux/Archive\_Models.pl} script archives models. To archive models use:
\begin{verbatim}
 ./scripts/aux/Archive_Models.pl <modelDirectory> <archiveDirectory>
\end{verbatim}
Here {\tt <modelDirectory>} should be the name of a directory containing \glc\ models as generated by the {\tt Run\_Galacticus.pl} script (see \S\ref{sec:RunningGrids}) and {\tt <archiveDirectory>} is a directory in which to archive the models. The specified model directory will be searched for models. For each model found, an XML file is created in the archive directory and will contain:
\begin{itemize}
 \item A full list of parameters used to run the model;
 \item The full version and revision data for \glc;
 \item The time at which the model was run;
 \item Any fitting data;
 \item Any comments associated with the model (i.e. anything placed in a file {\tt comments.txt} in the model directory);
 \item The name of a {\tt tar} archive containing any plots generated for the model.
\end{itemize}
Any plots found in the model directory will be combined into a {\tt tar} archive and stored in the archive directory.

